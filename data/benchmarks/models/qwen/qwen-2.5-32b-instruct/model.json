{
  "canonical_model_id": null,
  "fine_tuned_from_model_id": null,
  "name": "Qwen2.5 32B Instruct",
  "description": "Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",
  "release_date": "2024-09-19",
  "input_context_size": 131072,
  "output_context_size": 8192,
  "license": "apache-2.0",
  "multimodal": false,
  "web_hydrated": false,
  "knowledge_cutoff": null,
  "api_ref_link": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "playground_link": null,
  "paper_link": null,
  "scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5/",
  "repo_link": "https://github.com/QwenLM/Qwen2.5",
  "weights_link": "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct",
  "param_count": 32500000000,
  "training_tokens": 18000000000000,
  "qualitative_metrics": [
    {
      "dataset_name": "MMLU",
      "score": 0.833,
      "is_self_reported": true,
      "analysis_method": "MMLU benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MMLU-Pro",
      "score": 0.69,
      "is_self_reported": true,
      "analysis_method": "MMLU-Pro benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MMLU-Redux",
      "score": 0.839,
      "is_self_reported": true,
      "analysis_method": "MMLU-redux benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "BBH",
      "score": 0.845,
      "is_self_reported": true,
      "analysis_method": "BBH benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "ARC-C",
      "score": 0.704,
      "is_self_reported": true,
      "analysis_method": "ARC-C benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "TruthfulQA",
      "score": 0.578,
      "is_self_reported": true,
      "analysis_method": "TruthfulQA benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "Winogrande",
      "score": 0.82,
      "is_self_reported": true,
      "analysis_method": "Winogrande benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "HellaSwag",
      "score": 0.852,
      "is_self_reported": true,
      "analysis_method": "HellaSwag benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "GPQA",
      "score": 0.495,
      "is_self_reported": true,
      "analysis_method": "GPQA benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "TheoremQA",
      "score": 0.441,
      "is_self_reported": true,
      "analysis_method": "TheoremQA benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MATH",
      "score": 0.831,
      "is_self_reported": true,
      "analysis_method": "MATH benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MMLU-STEM",
      "score": 0.809,
      "is_self_reported": true,
      "analysis_method": "MMLU-STEM benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "GSM8K",
      "score": 0.959,
      "is_self_reported": true,
      "analysis_method": "GSM8K benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "HumanEval",
      "score": 0.884,
      "is_self_reported": true,
      "analysis_method": "HumanEval benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "HumanEval+",
      "score": 0.524,
      "is_self_reported": true,
      "analysis_method": "HumanEval+ benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MBPP",
      "score": 0.84,
      "is_self_reported": true,
      "analysis_method": "MBPP benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MBPP+",
      "score": 0.672,
      "is_self_reported": true,
      "analysis_method": "MBPP+ benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    },
    {
      "dataset_name": "MultiPL-E",
      "score": 0.754,
      "is_self_reported": true,
      "analysis_method": "MultiPL-E benchmark evaluation",
      "date_recorded": "2024-09-19",
      "source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/"
    }
  ]
}
